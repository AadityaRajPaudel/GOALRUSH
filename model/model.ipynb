{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eccd5dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.layers import Conv1D, Bidirectional, LSTM, Dense, Input, Dropout\n",
    "import numpy as np\n",
    "from tensorflow.keras.layers import SpatialDropout1D\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from collections import Counter\n",
    "\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58ee7cbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Negative</td>\n",
       "      <td>upset update Facebook texting might cry result...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Negative</td>\n",
       "      <td>Kenichan I dived many time ball Managed save 5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Negative</td>\n",
       "      <td>whole body feel itchy like fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Negative</td>\n",
       "      <td>nationwideclass behaving mad I see</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Negative</td>\n",
       "      <td>Kwesidei whole crew</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sentiment                                            message\n",
       "0  Negative  upset update Facebook texting might cry result...\n",
       "1  Negative  Kenichan I dived many time ball Managed save 5...\n",
       "2  Negative                    whole body feel itchy like fire\n",
       "3  Negative                 nationwideclass behaving mad I see\n",
       "4  Negative                                Kwesidei whole crew"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"cleaned_text_data.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e25dadc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1599999, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "137d566c",
   "metadata": {},
   "outputs": [],
   "source": [
    "smaller_used_data, rest_data = train_test_split(df, train_size = 0.2, stratify=df[\"sentiment\"], random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a4fffc54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(319999, 2)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smaller_used_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "68025bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = smaller_used_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "703d4566",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(319999, 2)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "21283f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data,test_data = train_test_split(df,test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5b7fcd88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(255999, 2)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ef338cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.DataFrame(train_data)\n",
    "test_data = pd.DataFrame(test_data)\n",
    "train_data['message'] = train_data['message'].astype(str)\n",
    "train_data['sentiment'] = train_data['sentiment'].astype(str)\n",
    "test_data['message'] = test_data['message'].astype(str)\n",
    "test_data['sentiment'] = test_data['sentiment'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a0656d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 20000\n",
    "oov_token = \"unknown\"\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_words, oov_token=oov_token)\n",
    "tokenizer.fit_on_texts(train_data['message'])\n",
    "\n",
    "tokenizer_json = tokenizer.to_json()\n",
    "with open('tokenizer.json', 'w') as json_file:\n",
    "    json_file.write(tokenizer_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9029b273",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[381, 395, 288, 806, 134, 346, 727, 48, 179, 10]]\n"
     ]
    }
   ],
   "source": [
    "print(str(tokenizer.texts_to_sequences(['There seems to be something wrong with the game today'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ba4f3777",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid= train_test_split(train_data['message'].tolist(),\n",
    "                                                      train_data['sentiment'].tolist(),\n",
    "                                                      test_size=0.1,\n",
    "                                                      stratify = train_data['sentiment'].tolist(),\n",
    "                                                      random_state=8)\n",
    "\n",
    "#further divided traindata into train data and validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "63f10cc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data len:230399\n",
      "Class distributionCounter({'Positive': 115347, 'Negative': 115052})\n",
      "Valid data len:25600\n",
      "Class distributionCounter({'Positive': 12816, 'Negative': 12784})\n"
     ]
    }
   ],
   "source": [
    "print('Train data len:'+str(len(X_train)))\n",
    "print('Class distribution'+str(Counter(y_train)))\n",
    "print('Valid data len:'+str(len(X_valid)))\n",
    "print('Class distribution'+ str(Counter(y_valid)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "92572ac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(230399, 50)\n",
      "(25600, 50)\n",
      "(64000, 50)\n"
     ]
    }
   ],
   "source": [
    "train_sequences = tokenizer.texts_to_sequences(X_train)\n",
    "valid_sequences = tokenizer.texts_to_sequences(X_valid)\n",
    "test_sequences = tokenizer.texts_to_sequences(test_data['message'].tolist())\n",
    "\n",
    "X_train = pad_sequences(train_sequences, padding='post', maxlen=50)\n",
    "x_valid = pad_sequences(valid_sequences, padding='post', maxlen=50)\n",
    "x_test = pad_sequences(test_sequences, padding='post', maxlen=50)\n",
    "\n",
    "\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "x_valid = np.array(x_valid)\n",
    "x_test = np.array(x_test)\n",
    "print(X_train.shape)\n",
    "print(x_valid.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "66a87f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "\n",
    "train_labels = le.fit_transform(y_train)\n",
    "train_labels = np.asarray( tf.keras.utils.to_categorical(train_labels))\n",
    "\n",
    "valid_labels = le.transform(y_valid)\n",
    "valid_labels = np.asarray( tf.keras.utils.to_categorical(valid_labels))\n",
    "\n",
    "test_labels = le.transform(test_data['sentiment'].tolist())\n",
    "test_labels = np.asarray(tf.keras.utils.to_categorical(test_labels))\n",
    "\n",
    "\n",
    "\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((X_train,train_labels))\n",
    "valid_ds = tf.data.Dataset.from_tensor_slices((x_valid,valid_labels))\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((x_test,test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c1679732",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([140, 136, 392, 578,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ae1801c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 50, 64)            1280064   \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, 48, 128)           24704     \n",
      "                                                                 \n",
      " global_max_pooling1d (Globa  (None, 128)              0         \n",
      " lMaxPooling1D)                                                  \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 128)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 2)                 258       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,305,026\n",
      "Trainable params: 1,305,026\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "number_of_classes = 2  \n",
    "\n",
    "max_features = 20000\n",
    "embedding_dim = 64\n",
    "sequence_length = 50\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Embedding(max_features + 1, embedding_dim, input_length=sequence_length,\n",
    "                                    embeddings_regularizer=regularizers.l2(0.0005)))\n",
    "\n",
    "model.add(tf.keras.layers.Conv1D(128, 3, activation='relu',\n",
    "                                 kernel_regularizer=regularizers.l2(0.0005),\n",
    "                                 bias_regularizer=regularizers.l2(0.0005)))\n",
    "\n",
    "model.add(tf.keras.layers.GlobalMaxPooling1D())\n",
    "\n",
    "model.add(tf.keras.layers.Dropout(0.5))\n",
    "\n",
    "\n",
    "model.add(tf.keras.layers.Dense(number_of_classes, activation='sigmoid',\n",
    "                                kernel_regularizer=regularizers.l2(0.001),\n",
    "                                bias_regularizer=regularizers.l2(0.001)))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "\n",
    "model.compile(loss=tf.keras.losses.CategoricalCrossentropy(from_logits=False), \n",
    "              optimizer='Nadam', \n",
    "              metrics=[\"CategoricalAccuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5389e72a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1800/1800 [==============================] - 50s 27ms/step - loss: 0.5822 - categorical_accuracy: 0.7444 - val_loss: 0.5581 - val_categorical_accuracy: 0.7585\n",
      "Epoch 2/20\n",
      "1800/1800 [==============================] - 45s 25ms/step - loss: 0.5571 - categorical_accuracy: 0.7590 - val_loss: 0.5527 - val_categorical_accuracy: 0.7620\n",
      "Epoch 3/20\n",
      "1800/1800 [==============================] - 48s 26ms/step - loss: 0.5510 - categorical_accuracy: 0.7622 - val_loss: 0.5474 - val_categorical_accuracy: 0.7663\n",
      "Epoch 4/20\n",
      "1800/1800 [==============================] - 45s 25ms/step - loss: 0.5469 - categorical_accuracy: 0.7653 - val_loss: 0.5441 - val_categorical_accuracy: 0.7680\n",
      "Epoch 5/20\n",
      "1800/1800 [==============================] - 44s 24ms/step - loss: 0.5430 - categorical_accuracy: 0.7665 - val_loss: 0.5414 - val_categorical_accuracy: 0.7694\n",
      "Epoch 6/20\n",
      "1800/1800 [==============================] - 45s 25ms/step - loss: 0.5408 - categorical_accuracy: 0.7687 - val_loss: 0.5401 - val_categorical_accuracy: 0.7711\n",
      "Epoch 7/20\n",
      "1800/1800 [==============================] - 44s 24ms/step - loss: 0.5387 - categorical_accuracy: 0.7703 - val_loss: 0.5393 - val_categorical_accuracy: 0.7707\n",
      "Epoch 8/20\n",
      "1800/1800 [==============================] - 44s 24ms/step - loss: 0.5383 - categorical_accuracy: 0.7709 - val_loss: 0.5384 - val_categorical_accuracy: 0.7706\n",
      "Epoch 9/20\n",
      "1800/1800 [==============================] - 43s 24ms/step - loss: 0.5381 - categorical_accuracy: 0.7713 - val_loss: 0.5371 - val_categorical_accuracy: 0.7726\n",
      "Epoch 10/20\n",
      "1800/1800 [==============================] - 43s 24ms/step - loss: 0.5366 - categorical_accuracy: 0.7712 - val_loss: 0.5373 - val_categorical_accuracy: 0.7711\n",
      "Epoch 11/20\n",
      "1800/1800 [==============================] - 44s 24ms/step - loss: 0.5360 - categorical_accuracy: 0.7723 - val_loss: 0.5364 - val_categorical_accuracy: 0.7714\n",
      "Epoch 12/20\n",
      "1800/1800 [==============================] - 46s 25ms/step - loss: 0.5353 - categorical_accuracy: 0.7735 - val_loss: 0.5376 - val_categorical_accuracy: 0.7729\n",
      "Epoch 13/20\n",
      "1800/1800 [==============================] - 45s 25ms/step - loss: 0.5340 - categorical_accuracy: 0.7742 - val_loss: 0.5373 - val_categorical_accuracy: 0.7735\n",
      "Epoch 14/20\n",
      "1800/1800 [==============================] - 45s 25ms/step - loss: 0.5332 - categorical_accuracy: 0.7747 - val_loss: 0.5369 - val_categorical_accuracy: 0.7727\n",
      "Epoch 15/20\n",
      "1800/1800 [==============================] - 46s 25ms/step - loss: 0.5329 - categorical_accuracy: 0.7759 - val_loss: 0.5368 - val_categorical_accuracy: 0.7742\n",
      "Epoch 16/20\n",
      "1800/1800 [==============================] - 44s 24ms/step - loss: 0.5321 - categorical_accuracy: 0.7765 - val_loss: 0.5360 - val_categorical_accuracy: 0.7740\n",
      "Epoch 17/20\n",
      "1800/1800 [==============================] - 46s 25ms/step - loss: 0.5318 - categorical_accuracy: 0.7761 - val_loss: 0.5367 - val_categorical_accuracy: 0.7738\n",
      "Epoch 18/20\n",
      "1800/1800 [==============================] - 45s 25ms/step - loss: 0.5315 - categorical_accuracy: 0.7767 - val_loss: 0.5410 - val_categorical_accuracy: 0.7716\n",
      "Epoch 19/20\n",
      "1800/1800 [==============================] - 45s 25ms/step - loss: 0.5314 - categorical_accuracy: 0.7771 - val_loss: 0.5362 - val_categorical_accuracy: 0.7739\n",
      "Epoch 20/20\n",
      "1800/1800 [==============================] - 44s 25ms/step - loss: 0.5309 - categorical_accuracy: 0.7775 - val_loss: 0.5363 - val_categorical_accuracy: 0.7746\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "history = model.fit(train_ds.shuffle(2000).batch(128),\n",
    "                    epochs= epochs ,\n",
    "                    validation_data=valid_ds.batch(128),\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a3729100",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('sentimentmodel.keras')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6c1caf7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('sentimentmodel.h5') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26601bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "max_features=20000\n",
    "tokenizer = Tokenizer(num_words=max_features)  \n",
    "json_string = tokenizer.to_json()\n",
    "with open('tokenizer.json', 'w') as outfile:\n",
    "    outfile.write(json_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ae7f8156",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.preprocessing.text.Tokenizer at 0x1c7dd499788>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907fcc0f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
